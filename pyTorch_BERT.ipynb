{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch-BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOF627tgn0Mi/FLo9ef8e4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FreakingPotato/DL_playground/blob/master/pyTorch_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adopted from https://zhuanlan.zhihu.com/p/477848486"
      ],
      "metadata": {
        "id": "hMnKnUXfKK54"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from random import *"
      ],
      "metadata": {
        "id": "VPEML7YFKEGo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *helper function*"
      ],
      "metadata": {
        "id": "DlneBJf8Kh8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for data preprocessing BERT can do two tasks: filling the word or predicting next word\n",
        "def make_data():\n",
        "    # we set the batch size to 6 which contains 3 positive cases and 3 negative cases\n",
        "    # according to the paper, BERT is completing two tasks together\n",
        "    # thefore, we assemble two sentences into one training cases\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
        "        # randomly selected two sentences from the paragraph\n",
        "        a_index, b_index = randrange(len(sentences)), randrange(len(sentences))\n",
        "        # find their token\n",
        "        tokens_a, tokens_b = token_list[a_index], token_list[b_index]\n",
        "        # adding special character\n",
        "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
        "        # segmentation id ???\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        # task one: filling the missing word\n",
        "        # masking sentence with 15% percentage, while the minimum word count is 1, max is 5\n",
        "        n_pred = min(max_pred, max(1, int(len(input_ids) * 0.15)))\n",
        "\n",
        "        # filter out candidate with special character\n",
        "        candidate_masked_position = [i for i, token in enumerate(input_ids)\n",
        "                                     if token != word2idx['[CLS]'] and token != word2idx['[SEP]']]\n",
        "   \n",
        "        shuffle(candidate_masked_position)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in candidate_masked_position[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            # three different masking strategies, with mask, change to a random word, do nothing\n",
        "            if random() < 0.8:\n",
        "                input_ids[pos] = word2idx['[MASK]']\n",
        "            elif random() > 0.9:  \n",
        "                index = randint(0, vocab_size - 1)\n",
        "                while index < 4:  # filter out special character\n",
        "                    index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = index\n",
        "\n",
        "\n",
        "        # zero padding the setnece\n",
        "        n_pad = max_len - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        # zero padding the prediction\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        # task 2: next setence prediction\n",
        "        if a_index + 1 == b_index and positive < batch_size / 2 :\n",
        "            # positive case\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
        "            positive += 1\n",
        "        elif a_index + 1 != b_index and negative < batch_size / 2 :\n",
        "            # negatice case\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
        "            negative += 1\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "# define the dataset\n",
        "class MyDataSet(Data.Dataset):\n",
        "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
        "        self.input_ids = input_ids\n",
        "        self.segment_ids = segment_ids\n",
        "        self.masked_tokens = masked_tokens\n",
        "        self.masked_pos = masked_pos\n",
        "        self.isNext = isNext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
        "\n",
        "\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, seq_len = seq_q.size()\n",
        "    # eq(zero) is PAD token\n",
        "    # eq(0) is the token of PAD\n",
        "    # 下面的操作可以选出pad是True,不是pad是False\n",
        "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)\n",
        "    # [batch_size, 1, seq_len] 内容是 False,False,..., True这样的\n",
        "    # expand用于张量的复制操作\n",
        "    return pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\n",
        "\n",
        "\n",
        "# activation function gelu\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "          Implementation of the gelu activation function.\n",
        "          For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "          0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "          Also see https://arxiv.org/abs/1606.08415\n",
        "        \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        # generating serials number with positional information \n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)\n",
        "        # [seq_len] -> [batch_size, seq_len]\n",
        "        # token and seg can be directly used, but pos need manually add positional information?? \n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)\n",
        "\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # attn_mask: attention_mask label the word need attention and discard word with mask\n",
        "        #  Q,K,V data size：[batch_size, n_heads, seq_len, d_k]\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
        "        # scores : [batch_size, n_heads, seq_len, seq_len] seq_len maximum number is 30\n",
        "        # QK production will get attention score, since mask location need to be discard, so we need to add a small value to those location(which will equal to zero after softmax operation)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        # softmax for attentino sum euqals to 1\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        # weighted attention score\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size, seq_len, d_model],\n",
        "        # k: [batch_size, seq_len, d_model],\n",
        "        # v: [batch_size, seq_len, d_model]\n",
        "        # since Q,K,V are equls, the residual can be any of values\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # 经过三层映射并根据多头注意力改变一下维度形式 ???\n",
        "        # [batch_size, n_heads, seq_len, d_k]\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n",
        "\n",
        "        # 对每个头应该重复一下attention_mask的位置 ???\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
        "\n",
        "        # object declare and usage at the same step\n",
        "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)  \n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_k)\n",
        "        # context: [batch_size, seq_len, n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual)\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(gelu(self.fc1(x)))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
        "        enc_outputs = self.pos_ffn(enc_outputs)\n",
        "        return enc_outputs\n",
        "\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Sequential(nn.Linear(d_model, d_model), nn.Dropout(0.5), nn.Tanh(),)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        # fc2 is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.fc2.weight = embed_weight\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, enc_self_attn_mask)\n",
        "\n",
        "        # 下一个句子预测任务\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.fc(output[:, 0])\n",
        "        # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled)\n",
        "        # [batch_size, 2] predict isNext\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model)\n",
        "        # [batch_size, max_pred, d_model]\n",
        "        # 关于gather函数：对tensor做聚合\n",
        "        h_masked = torch.gather(output, 1, masked_pos)  # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.activ2(self.linear(h_masked))\n",
        "        logits_lm = self.fc2(h_masked)\n",
        "        # [batch_size, max_pred, vocab_size]\n",
        "        return logits_lm, logits_clsf\n"
      ],
      "metadata": {
        "id": "SPYhviY2Kfvu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLpdqOcVJ9tD",
        "outputId": "b35f516f-2ff1-47ad-bbfa-acddd32b1c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0010 loss = 1.240105\n",
            "Epoch: 0020 loss = 0.886013\n",
            "Epoch: 0030 loss = 0.819673\n",
            "Epoch: 0040 loss = 0.745577\n",
            "Epoch: 0050 loss = 0.702412\n",
            "Epoch: 0060 loss = 0.725242\n",
            "Epoch: 0070 loss = 0.730126\n",
            "Epoch: 0080 loss = 0.663990\n",
            "Epoch: 0090 loss = 0.715929\n",
            "Epoch: 0100 loss = 0.668781\n",
            "Epoch: 0110 loss = 0.680513\n",
            "Epoch: 0120 loss = 0.666867\n",
            "Epoch: 0130 loss = 0.696233\n",
            "Epoch: 0140 loss = 0.720583\n",
            "Epoch: 0150 loss = 0.722957\n",
            "Epoch: 0160 loss = 0.675883\n",
            "Epoch: 0170 loss = 0.691234\n",
            "Epoch: 0180 loss = 0.704149\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    text = (\n",
        "        'Hello, how are you? I am Romeo.\\n'  # R\n",
        "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'  # J\n",
        "        'Nice meet you too. How are you today?\\n'  # R\n",
        "        'Great. My baseball team won the competition.\\n'  # J\n",
        "        'Oh Congratulations, Juliet\\n'  # R\n",
        "        'Thank you Romeo\\n'  # J\n",
        "        'Where are you going today?\\n'  # R\n",
        "        'I am going shopping. What about you?\\n'  # J\n",
        "        'I am going to visit my grandmother. she is not very well'  # R\n",
        "    )\n",
        "\n",
        "\n",
        "    # Preprocessing the input\n",
        "    # remove punctuation and replace with space\n",
        "    sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')\n",
        "    # build the vocabulary\n",
        "    word_list = list(set(\" \".join(sentences).split()))\n",
        "    # project the vocabulary to the index\n",
        "    word2idx = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "    for i, w in enumerate(word_list):\n",
        "        word2idx[w] = i + 4\n",
        "    idx2word = {i: w for i, w in enumerate(word2idx)}\n",
        "    vocab_size = len(word2idx)\n",
        "\n",
        "    # transfer sentence into token\n",
        "    token_list = list()\n",
        "    for sentence in sentences:\n",
        "        every_word = [word2idx[s] for s in sentence.split()]\n",
        "        token_list.append(every_word)\n",
        "\n",
        "    # model hyperparameter setup\n",
        "    max_len = 30   # max input length\n",
        "    batch_size = 6\n",
        "    max_pred = 5  # maximum word prediction\n",
        "    n_layers = 6  \n",
        "    n_heads = 12  # number of attention head\n",
        "    d_model = 768  # dimension of embedding layer\n",
        "    d_ff = 768 * 4  # dimeension of feed forward layer\n",
        "    d_k = d_v = 64  # KQ维度d_k, V的维度是d_V 维度*头数 = 768 ???\n",
        "    n_segments = 2  # sentence segmentation \n",
        "\n",
        "    batch = make_data()\n",
        "    # zip function can take the iterable object as parameters, and can pack the element in the object as turple and return those turple as list\n",
        "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
        "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
        "        torch.LongTensor(input_ids), torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens), \\\n",
        "        torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
        "\n",
        "    dataloader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)\n",
        "\n",
        "    model = BERT()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(180):\n",
        "        for input_ids, segment_ids, masked_tokens, masked_pos, isNext in dataloader:\n",
        "            logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "            loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1))  # for masked LM\n",
        "            loss_lm = (loss_lm.float()).mean()\n",
        "            loss_clsf = criterion(logits_clsf, isNext)  # for sentence classification\n",
        "            loss = loss_lm + loss_clsf\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[0]\n",
        "print(text)\n",
        "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list : ', [pos for pos in masked_tokens if pos != 0])\n",
        "print('predict masked tokens list : ', [pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ', True if logits_clsf else False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrgoLjWcKVeq",
        "outputId": "71ea79d1-2017-46f5-d77b-41d5d58b18e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you? I am Romeo.\n",
            "Hello, Romeo My name is Juliet. Nice to meet you.\n",
            "Nice meet you too. How are you today?\n",
            "Great. My baseball team won the competition.\n",
            "Oh Congratulations, Juliet\n",
            "Thank you Romeo\n",
            "Where are you going today?\n",
            "I am going shopping. What about you?\n",
            "I am going to visit my grandmother. she is not very well\n",
            "['[CLS]', 'hello', 'romeo', 'my', 'name', 'is', 'juliet', 'nice', 'to', 'meet', 'you', '[SEP]', 'i', 'am', 'going', 'to', 'visit', '[MASK]', 'grandmother', 'she', 'is', 'not', 'very', '[MASK]', '[SEP]']\n",
            "masked tokens list :  [27, 10, 5]\n",
            "predict masked tokens list :  [27, 10, 5]\n",
            "isNext :  False\n",
            "predict isNext :  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bVbMDrBZTBbl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}