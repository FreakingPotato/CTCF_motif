{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch-BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOeAsa0HAjVPtxNvcJnkxwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FreakingPotato/DL_playground/blob/master/pyTorch_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adopted from https://zhuanlan.zhihu.com/p/477848486"
      ],
      "metadata": {
        "id": "hMnKnUXfKK54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from random import *"
      ],
      "metadata": {
        "id": "VPEML7YFKEGo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *helper function*"
      ],
      "metadata": {
        "id": "DlneBJf8Kh8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据处理部分,包含bert的两个任务，完形填空和下个句子预测\n",
        "# for data preprocessing BERT can do two tasks: filling the word or predicting next word\n",
        "def make_data():\n",
        "    # 我们设置的batch_size是6，三个正样本，三个负样本\n",
        "    # 根据论文bert同时做完形填空和下个句子预测两个任务，\n",
        "    # 所以一条训练数据应该是文本里面抽取两个句子，拼成一个训练样本\n",
        "    # we set the batch size to 6 which contains 3 positive cases and 3 negative cases\n",
        "    # according to the paper, BERT is completing two tasks together\n",
        "    # thefore, we assemble two sentences into one training cases\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
        "        # 构造样本首先得在文本里选两个句子\n",
        "        # randomly selected two sentences from the paragraph\n",
        "        a_index, b_index = randrange(len(sentences)), randrange(len(sentences))\n",
        "        # 用下标把这俩句子的索引形式拿出来\n",
        "        # find their token\n",
        "        tokens_a, tokens_b = token_list[a_index], token_list[b_index]\n",
        "        # 按论文，添加上特殊字符\n",
        "        # adding special character\n",
        "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
        "        # 区分上下句子的分割符号\n",
        "        # segmentation id ???\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        # 第一个任务，完形填空\n",
        "        # 对一条数据中多少个词做mask，原论文是取15%的概率,限制范围设置其最小是1，最大是5\n",
        "        # task one: filling the missing word\n",
        "        # masking sentence with 15% percentage, while the minimum word count is 1, max is 5\n",
        "        n_pred = min(max_pred, max(1, int(len(input_ids) * 0.15)))\n",
        "        # 知道要mask几个词后，去确定这几个词是啥，也就是确定索引是啥\n",
        "        # 先确定候选的范围，cls，sep啥的不能做mask\n",
        "        # filter out candidate with special character\n",
        "        candidate_masked_position = [i for i, token in enumerate(input_ids)\n",
        "                                     if token != word2idx['[CLS]'] and token != word2idx['[SEP]']]\n",
        "        # 打乱之后抽前面的n_pred个\n",
        "        shuffle(candidate_masked_position)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in candidate_masked_position[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            # 根据论文对mask做三种不同的处理，不做处理不用\n",
        "            # three different masking strategies, with mask, change to a random word, do nothing\n",
        "            if random() < 0.8:\n",
        "                input_ids[pos] = word2idx['[MASK]']\n",
        "            elif random() > 0.9:  \n",
        "                index = randint(0, vocab_size - 1)\n",
        "                while index < 4:  # filter out special character\n",
        "                    index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = index\n",
        "\n",
        "        # 做完mask处理应该添加zero padding\n",
        "        # zero padding the setnece\n",
        "        n_pad = max_len - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        # 对masked_tokens, masked_pos 做pad操作\n",
        "        # zero padding the prediction\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        # 第二个任务，下一个句子预测\n",
        "        # task 2: next setence prediction\n",
        "        if a_index + 1 == b_index and positive < batch_size / 2 :\n",
        "            # positive case\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
        "            positive += 1\n",
        "        elif a_index + 1 != b_index and negative < batch_size / 2 :\n",
        "            # negatice case\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
        "            negative += 1\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "# define the dataset\n",
        "class MyDataSet(Data.Dataset):\n",
        "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
        "        self.input_ids = input_ids\n",
        "        self.segment_ids = segment_ids\n",
        "        self.masked_tokens = masked_tokens\n",
        "        self.masked_pos = masked_pos\n",
        "        self.isNext = isNext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
        "\n",
        "\n",
        "# 模型搭建部分, 就是transformer的encoder\n",
        "# BERT arch: transfomer encoder part\n",
        "\n",
        "# 此函数的功能: 返回需要做attention的位置列表\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, seq_len = seq_q.size()\n",
        "    # eq(zero) is PAD token\n",
        "    # eq(0) is the token of PAD\n",
        "    # 下面的操作可以选出pad是True,不是pad是False\n",
        "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)\n",
        "    # [batch_size, 1, seq_len] 内容是 False,False,..., True这样的\n",
        "    # expand用于张量的复制操作\n",
        "    return pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\n",
        "\n",
        "\n",
        "# activation function gelu\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "          Implementation of the gelu activation function.\n",
        "          For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "          0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "          Also see https://arxiv.org/abs/1606.08415\n",
        "        \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        # torch.arange产生序列数字0,1,2,3这种\n",
        "        # generating serials number with positional information \n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)\n",
        "        # [seq_len] -> [batch_size, seq_len]\n",
        "        # token是直接索引映射，pos是根据0,1,2,3映射，seg是很具0,1 两种判断上下句子\n",
        "        # token and seg can be directly used, but pos need manually add positional information?? \n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)\n",
        "\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # attn_mask: attention_mask label the word need attention and discard word with mask\n",
        "        #  Q,K,V data size：[batch_size, n_heads, seq_len, d_k]\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
        "        # scores : [batch_size, n_heads, seq_len, seq_len] seq_len maximum number is 30\n",
        "        # QK相乘除d_k得到注意力,但是在pad位置不能算权重，就给这些位置加上绝对值很大的复数，softmax之后对应的权重项就接近0了\n",
        "        # QK production will get attention score, since mask location need to be discard, so we need to add a small value to those location(which will equal to zero after softmax operation)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        # softmax for attentino sum euqals to 1\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        # weighted attention score\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size, seq_len, d_model],\n",
        "        # k: [batch_size, seq_len, d_model],\n",
        "        # v: [batch_size, seq_len, d_model]\n",
        "        # since Q,K,V are equls, the residual can be any of values\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # 经过三层映射并根据多头注意力改变一下维度形式 ???\n",
        "        # [batch_size, n_heads, seq_len, d_k]\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n",
        "\n",
        "        # 对每个头应该重复一下attention_mask的位置 ???\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
        "\n",
        "        # object declare and usage at the same step\n",
        "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)  \n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_k)\n",
        "        # context: [batch_size, seq_len, n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual)\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(gelu(self.fc1(x)))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
        "        enc_outputs = self.pos_ffn(enc_outputs)\n",
        "        return enc_outputs\n",
        "\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Sequential(nn.Linear(d_model, d_model), nn.Dropout(0.5), nn.Tanh(),)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        # fc2 is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.fc2.weight = embed_weight\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, enc_self_attn_mask)\n",
        "\n",
        "        # 下一个句子预测任务\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.fc(output[:, 0])\n",
        "        # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled)\n",
        "        # [batch_size, 2] predict isNext\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model)\n",
        "        # [batch_size, max_pred, d_model]\n",
        "        # 关于gather函数：对tensor做聚合\n",
        "        h_masked = torch.gather(output, 1, masked_pos)  # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.activ2(self.linear(h_masked))\n",
        "        logits_lm = self.fc2(h_masked)\n",
        "        # [batch_size, max_pred, vocab_size]\n",
        "        return logits_lm, logits_clsf\n"
      ],
      "metadata": {
        "id": "SPYhviY2Kfvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLpdqOcVJ9tD",
        "outputId": "50b1e872-2898-431e-bce5-07342692c0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0010 loss = 1.532707\n",
            "Epoch: 0020 loss = 1.068846\n",
            "Epoch: 0030 loss = 0.947468\n",
            "Epoch: 0040 loss = 0.991505\n",
            "Epoch: 0050 loss = 0.842801\n",
            "Epoch: 0060 loss = 0.910206\n",
            "Epoch: 0070 loss = 0.940406\n",
            "Epoch: 0080 loss = 0.914088\n",
            "Epoch: 0090 loss = 0.884946\n",
            "Epoch: 0100 loss = 0.837120\n",
            "Epoch: 0110 loss = 0.867682\n",
            "Epoch: 0120 loss = 0.847013\n",
            "Epoch: 0130 loss = 0.847873\n",
            "Epoch: 0140 loss = 0.855263\n",
            "Epoch: 0150 loss = 0.888119\n",
            "Epoch: 0160 loss = 0.860719\n",
            "Epoch: 0170 loss = 0.965066\n",
            "Epoch: 0180 loss = 0.888949\n",
            "Hello, how are you? I am Romeo.\n",
            "Hello, Romeo My name is Juliet. Nice to meet you.\n",
            "Nice meet you too. How are you today?\n",
            "Great. My baseball team won the competition.\n",
            "Oh Congratulations, Juliet\n",
            "Thank you Romeo\n",
            "Where are you going today?\n",
            "I am going shopping. What about you?\n",
            "I am going to visit my grandmother. she is not very well\n",
            "['[CLS]', 'i', 'am', 'going', 'to', 'visit', '[MASK]', 'grandmother', 'she', 'is', '[MASK]', 'very', 'well', '[SEP]', 'where', 'are', 'you', '[MASK]', 'today', '[SEP]']\n",
            "masked tokens list :  [28, 10, 14]\n",
            "predict masked tokens list :  [28, 10, 14]\n",
            "isNext :  False\n",
            "predict isNext :  True\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    # 简易的手动输入文本做数据集\n",
        "    text = (\n",
        "        'Hello, how are you? I am Romeo.\\n'  # R\n",
        "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'  # J\n",
        "        'Nice meet you too. How are you today?\\n'  # R\n",
        "        'Great. My baseball team won the competition.\\n'  # J\n",
        "        'Oh Congratulations, Juliet\\n'  # R\n",
        "        'Thank you Romeo\\n'  # J\n",
        "        'Where are you going today?\\n'  # R\n",
        "        'I am going shopping. What about you?\\n'  # J\n",
        "        'I am going to visit my grandmother. she is not very well'  # R\n",
        "    )\n",
        "\n",
        "    # 对词表进行处理\n",
        "    # re.sub用法：用 '' 替换文本里的给定字符\n",
        "    sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')\n",
        "    # 建立词表\n",
        "    word_list = list(set(\" \".join(sentences).split()))\n",
        "    # 把自然语言词汇映射成索引\n",
        "    word2idx = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "    for i, w in enumerate(word_list):\n",
        "        word2idx[w] = i + 4\n",
        "    idx2word = {i: w for i, w in enumerate(word2idx)}\n",
        "    vocab_size = len(word2idx)\n",
        "\n",
        "    # 每一句话都变成索引形式\n",
        "    token_list = list()\n",
        "    for sentence in sentences:\n",
        "        every_word = [word2idx[s] for s in sentence.split()]\n",
        "        token_list.append(every_word)\n",
        "\n",
        "    # 模型参数设置\n",
        "    max_len = 30   # 规定同一个batch里面都由30个token组成，不够补pad\n",
        "    batch_size = 6\n",
        "    max_pred = 5  # 最多需要预测多少单词，应用于第一个完形填空任务\n",
        "    n_layers = 6  # 几个基本单元\n",
        "    n_heads = 12  # 多头注意力的头数\n",
        "    d_model = 768  # embedding的维度，三种embedding是一样的\n",
        "    d_ff = 768 * 4  # encoder单元里面feed forward全连接层的维度\n",
        "    d_k = d_v = 64  # KQ维度d_k, V的维度是d_V 维度*头数 = 768\n",
        "    n_segments = 2  # 区分上下两句\n",
        "\n",
        "    batch = make_data()\n",
        "    # zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。\n",
        "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
        "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
        "        torch.LongTensor(input_ids), torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens), \\\n",
        "        torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
        "\n",
        "    dataloader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)\n",
        "\n",
        "    model = BERT()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
        "\n",
        "    # 训练\n",
        "    for epoch in range(180):\n",
        "        for input_ids, segment_ids, masked_tokens, masked_pos, isNext in dataloader:\n",
        "            logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "            loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1))  # for masked LM\n",
        "            loss_lm = (loss_lm.float()).mean()\n",
        "            loss_clsf = criterion(logits_clsf, isNext)  # for sentence classification\n",
        "            loss = loss_lm + loss_clsf\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # 测试\n",
        "    # Predict mask tokens ans isNext\n",
        "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[0]\n",
        "    print(text)\n",
        "    print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n",
        "\n",
        "    logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
        "    logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "    print('masked tokens list : ', [pos for pos in masked_tokens if pos != 0])\n",
        "    print('predict masked tokens list : ', [pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "    logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "    print('isNext : ', True if isNext else False)\n",
        "    print('predict isNext : ', True if logits_clsf else False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nrgoLjWcKVeq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}